{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "#https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', 'val']\n"
     ]
    }
   ],
   "source": [
    "IMG_PATH = \"data/\"\n",
    "print(os.listdir(IMG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = IMG_PATH + \"train/\"\n",
    "validation_path = IMG_PATH + \"val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_imgs(path):\n",
    "    print(path)\n",
    "    for value in os.listdir(path):\n",
    "        print(value, \"has\", len(os.listdir(path + value)), \"imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train/\n",
      "NORMAL has 1326 imgs\n",
      "PNEUMONIA has 3860 imgs\n",
      "data/val/\n",
      "non_pneumonia has 35 imgs\n",
      "pneumonia has 35 imgs\n"
     ]
    }
   ],
   "source": [
    "number_of_imgs(train_path)\n",
    "number_of_imgs(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "nb_train_samples = 5218\n",
    "nb_validation_samples = 75\n",
    "epochs = 10\n",
    "batch_size = 35\n",
    "checkpoint_filepath = \"checkpoint/model_val_loss-{val_loss:.2f}.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 12:55:38.786608 139938689468160 deprecation_wrapper.py:119] From /home/h8953/tensorflow_gpu_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0827 12:55:38.798352 139938689468160 deprecation_wrapper.py:119] From /home/h8953/tensorflow_gpu_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0827 12:55:38.800515 139938689468160 deprecation_wrapper.py:119] From /home/h8953/tensorflow_gpu_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0827 12:55:38.820404 139938689468160 deprecation_wrapper.py:119] From /home/h8953/tensorflow_gpu_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0827 12:55:39.048618 139938689468160 deprecation_wrapper.py:119] From /home/h8953/tensorflow_gpu_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0827 12:55:39.049222 139938689468160 deprecation_wrapper.py:119] From /home/h8953/tensorflow_gpu_env/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())                                    \n",
    "model.add(Dense(64))                                    \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))                                     \n",
    "model.add(Activation('sigmoid'))            \n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 16,320,449\n",
      "Trainable params: 16,320,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5184 images belonging to 2 classes.\n",
      "Found 68 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "149/149 [==============================] - 103s 691ms/step - loss: 0.2745 - acc: 0.8823 - val_loss: 0.3540 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35405, saving model to checkpoint/model_val_loss-0.35.h5\n",
      "Epoch 2/10\n",
      "149/149 [==============================] - 97s 651ms/step - loss: 0.1663 - acc: 0.9373 - val_loss: 0.3059 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35405 to 0.30592, saving model to checkpoint/model_val_loss-0.31.h5\n",
      "Epoch 3/10\n",
      "149/149 [==============================] - 98s 659ms/step - loss: 0.1364 - acc: 0.9494 - val_loss: 0.1076 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.30592 to 0.10760, saving model to checkpoint/model_val_loss-0.11.h5\n",
      "Epoch 4/10\n",
      "149/149 [==============================] - 100s 672ms/step - loss: 0.1125 - acc: 0.9580 - val_loss: 0.1069 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10760 to 0.10694, saving model to checkpoint/model_val_loss-0.11.h5\n",
      "Epoch 5/10\n",
      "149/149 [==============================] - 99s 666ms/step - loss: 0.1134 - acc: 0.9578 - val_loss: 0.0421 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10694 to 0.04211, saving model to checkpoint/model_val_loss-0.04.h5\n",
      "Epoch 6/10\n",
      "149/149 [==============================] - 99s 664ms/step - loss: 0.0977 - acc: 0.9649 - val_loss: 0.1445 - val_acc: 0.9412\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04211\n",
      "Epoch 7/10\n",
      "149/149 [==============================] - 99s 666ms/step - loss: 0.0874 - acc: 0.9687 - val_loss: 0.0877 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04211\n",
      "Epoch 8/10\n",
      "149/149 [==============================] - 99s 666ms/step - loss: 0.0784 - acc: 0.9722 - val_loss: 0.0316 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.04211 to 0.03161, saving model to checkpoint/model_val_loss-0.03.h5\n",
      "Epoch 9/10\n",
      "149/149 [==============================] - 99s 667ms/step - loss: 0.0761 - acc: 0.9767 - val_loss: 0.0466 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.03161\n",
      "Epoch 10/10\n",
      "149/149 [==============================] - 100s 668ms/step - loss: 0.1004 - acc: 0.9641 - val_loss: 0.1289 - val_acc: 0.9265\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f457a839ac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    rescale=1. / 255,                   #Scaling RGB values from \"0-255\" to \"0-1\" \n",
    "    zoom_range=0.2,                 #Random zoom between 0 and 0.2\n",
    "    horizontal_flip=True) #Random horizontal flip\n",
    "               \n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255) #Scaling RGB values from \"0-255\" to \"0-1\". \n",
    "\n",
    "\n",
    "# Generates batches of image data for training\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,                                       #Train data directory\n",
    "    target_size=(img_width, img_height),                  #Image size\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\")                                  #Amount of classes, binary if 2 classes, else categorical\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_path,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"binary\")\n",
    "\n",
    "\n",
    "# Lowering learning rate if no progress is made.\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "# Defining checkpoint that saves the best model or model\"s weights based on the value that we are monitoring.\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    #save_weights_only=True,\n",
    "    mode=\"min\")\n",
    "\n",
    "\n",
    "# Defining callbacks which includes checkpoint and reducing learning rate on plateau.\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# Training CNN\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,         \n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size,\n",
    "    callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu_env",
   "language": "python",
   "name": "tensorflow_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
